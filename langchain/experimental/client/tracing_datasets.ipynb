{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4596ea-a631-416d-a2a4-3577c140493d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tracing and Datasets with LangChainPlus\n",
    "\n",
    "LangChain makes it easy to get started with Agents and other LLM applications. However, it can be tricky to get right, especially when you need to deliver a full product. To speed up your application development process, and to help monitor your applications in production, LangChain offers additional tracing and tooling.\n",
    "\n",
    "When might you want to use tracing? Some situations we've found it useful include:\n",
    "- Quickly debugging a new chain, agent, or set of tools\n",
    "- Evaluating a given chain across different LLMs or Chat Models to compare results or improve prompts\n",
    "- Running a given chain multiple time on a dataset to ensure it consistently meets a quality bar.\n",
    "\n",
    "\n",
    "In this notebook, we'll show how to enable tracing in your LangChain applications and walk you a couple common ways to evaluate your agents.\n",
    "We'll focus on using Datasets to benchmark Chain behavior.\n",
    "\n",
    "**Bear in mind that this notebook is designed under the assumption that you're running the latest LangChain+ server locally in the background. This is done using the folowing command in your terminal:**\n",
    "\n",
    "\n",
    "```\n",
    "pip install --upgrade langchain\n",
    "langchain plus start\n",
    "```\n",
    "\n",
    "We also have a hosted version which is in private beta. We will share more details as it progresses.\n",
    "\n",
    "Now, let's get started by creating a client to connect to LangChain+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77d064-41b4-41fb-82e6-2d16461269ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up Tracing\n",
    "\n",
    "The V2 tracing API can be activated by setting the `LANGCHAIN_TRACING_V2` environment variable to true. Assuming you've successfully initiated the server as described earlier, running LangChain Agents, Chains, LLMs, and other primitives will automatically start capturing traces. Let's begin our exploration with a straightforward math example.\n",
    "\n",
    "**NOTE**: You must also set your `OPENAI_API_KEY` and `SERPAPI_API_KEY` environment variables in order to run the following tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935e832-9ae1-4557-8d08-890c425f18e2",
   "metadata": {},
   "source": [
    "**NOTE:** You can also use the `tracing_v2_enabled` context manager to capture sessions within a given context:\n",
    "```\n",
    "from langchain.callbacks.manager import tracing_v2_enabled\n",
    "with tracing_v2_enabled(\"My Session Name\"):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b638a-2ae4-46ef-bf6a-d59bf85d587f",
   "metadata": {},
   "source": [
    "**NOTE:** You can optionally set the `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables if using the hosted version which is in private beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904db9a5-f387-4a57-914c-c8af8d39e249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can click the link below to view the UI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost\", target=\"_blank\" rel=\"noopener\">LangChain+ Client</a>"
      ],
      "text/plain": [
       "LangChainPlusClient (API URL: http://localhost:8000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.client import LangChainPlusClient\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_SESSION\"] = \"Tracing Walkthrough\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://langchainpro-api-gateway-12bfv6cf.uc.gateway.dev\"  # Uncomment this line if you want to use the hosted version\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-LANGCHAINPLUS-API-KEY>\"  # Uncomment this line if you want to use the hosted version.\n",
    "\n",
    "client = LangChainPlusClient()\n",
    "print(\"You can click the link below to view the UI\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c801853-8e96-404d-984c-51ace59cbbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19537902-b95c-4390-80a4-f6c9a937081e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown format from LLM: Sorry, I cannot answer this question as it requires information from the future.\n",
      "LLMMathChain._evaluate(\"\n",
      "(age ** 0.43)\n",
      "\") raised error: 'age'. Please try again with a valid numerical expression\n",
      "LLMMathChain._evaluate(\"\n",
      "(total number of points scored in the 2023 super bowl)**0.23\n",
      "\") raised error: invalid syntax. Perhaps you forgot a comma? (<expr>, line 1). Please try again with a valid numerical expression\n",
      "LLMMathChain._evaluate(\"\n",
      "round(0.2791714614499425, 2)\n",
      "\") raised error: 'VariableNode' object is not callable. Please try again with a valid numerical expression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} <CIMultiDictProxy('Date': 'Wed, 24 May 2023 02:33:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7cc219f30c1d421c-EWR', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400')>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['39,566,248',\n",
       " \"Anwar Hadid's age raised to the 0.43 power is approximately 3.87.\",\n",
       " ValueError('LLMMathChain._evaluate(\"\\n(age ** 0.43)\\n\") raised error: \\'age\\'. Please try again with a valid numerical expression'),\n",
       " 'The distance between Paris and Boston is 3448 miles.',\n",
       " ValueError('unknown format from LLM: Sorry, I cannot answer this question as it requires information from the future.'),\n",
       " ValueError('LLMMathChain._evaluate(\"\\n(total number of points scored in the 2023 super bowl)**0.23\\n\") raised error: invalid syntax. Perhaps you forgot a comma? (<expr>, line 1). Please try again with a valid numerical expression'),\n",
       " '3 points were scored more in the 2023 Super Bowl than in the 2022 Super Bowl.',\n",
       " '1.9347796717823205',\n",
       " '1.2600077141429156',\n",
       " ValueError('LLMMathChain._evaluate(\"\\nround(0.2791714614499425, 2)\\n\") raised error: \\'VariableNode\\' object is not callable. Please try again with a valid numerical expression')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "inputs = [\n",
    "    \"How many people live in canada as of 2023?\",\n",
    "    \"who is dua lipa's boyfriend? what is his age raised to the .43 power?\",\n",
    "    \"what is dua lipa's boyfriend age raised to the .43 power?\",\n",
    "    \"how far is it from paris to boston in miles\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl? what is that number raised to the .23 power?\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl raised to the .23 power?\",\n",
    "    \"how many more points were scored in the 2023 super bowl than in the 2022 super bowl?\",\n",
    "    \"what is 153 raised to .1312 power?\",\n",
    "    \"who is kendall jenner's boyfriend? what is his height (in inches) raised to .13 power?\",\n",
    "    \"what is 1213 divided by 4345?\",\n",
    "]\n",
    "results = []\n",
    "\n",
    "async def arun(agent, input_example):\n",
    "    try:\n",
    "        return await agent.arun(input_example)\n",
    "    except Exception as e:\n",
    "        # The agent sometimes makes mistakes! These will be captured by the tracing.\n",
    "        print(e)\n",
    "        return e\n",
    "for input_example in inputs:\n",
    "    results.append(arun(agent, input_example))\n",
    "await asyncio.gather(*results)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43c311-4e09-4d57-9ef3-13afb96ff430",
   "metadata": {},
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "Now that you've captured a session entitled 'Tracing Walkthrough', it's time to create a dataset. We will do so using the `create_dataset` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14a9881-2a01-404c-8c56-0b78565c3ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"calculator-example-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17580c4b-bd04-4dde-9d21-9d4edd25b00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name in set([dataset.name for dataset in client.list_datasets()]):\n",
    "    client.delete_dataset(dataset_name=dataset_name)\n",
    "dataset = client.create_dataset(dataset_name, description=\"A calculator example dataset\")\n",
    "runs = client.list_runs(\n",
    "    session_name=os.environ[\"LANGCHAIN_SESSION\"],\n",
    "    execution_order=1, # Only return the top-level runs\n",
    "    error=False, # Only runs that succeed\n",
    ")\n",
    "for run in runs:\n",
    "    client.create_example(inputs=run.inputs, outputs=run.outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79dea2-fbaa-4c12-9083-f6154b51e2d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Alternative: Creating a Dataset in the UI** \n",
    "\n",
    "Alternatively, you could create or edit the dataset in the UI using the following steps:\n",
    "\n",
    "   1. Navigate to the UI by clicking on the link below.\n",
    "   2. Select the 'search_and_math_chain' session from the list.\n",
    "   3. Next to the fist example, click \"+ to Dataset\".\n",
    "   4. Click \"Create Dataset\" and create a title **\"calculator-example-dataset\"**.\n",
    "   5. Add the other examples to the dataset as well\n",
    "\n",
    "Once you've used LangChain+ for a while, you will have a number of datasets to work with. To view all saved datasets, execute the following code:\n",
    "\n",
    "```\n",
    "datasets = client.list_datasets()\n",
    "print(datasets)\n",
    "```\n",
    "\n",
    "\n",
    "**Optional:** If you didn't run the trace above, you can also create datasets by uploading dataframes or CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1baa677c-5642-4378-8e01-3aa1647f19d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install datasets > /dev/null\n",
    "# !pip install pandas > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d14593-c61f-449f-a38f-772ca43707c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain.evaluation.loading import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"agent-search-calculator\")\n",
    "# df = pd.DataFrame(dataset, columns=[\"question\", \"answer\"])\n",
    "# df.columns = [\"input\", \"output\"] # The chain we want to evaluate below expects inputs with the \"input\" key \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a7ea76-79ca-4765-abf7-231e884040d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_name = \"calculator-example-dataset\"\n",
    "\n",
    "# if dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
    "#     dataset = client.upload_dataframe(df, \n",
    "#                             name=dataset_name,\n",
    "#                             description=\"A calculator example dataset\",\n",
    "#                             input_keys=[\"input\"],\n",
    "#                             output_keys=[\"output\"],\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07885b10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running a Chain on a Traced Dataset\n",
    "\n",
    "Once you have a dataset, you can run a compatible chain or other object over it to see its results. The run traces will automatically be associated with the dataset for easy attribution and analysis.\n",
    "\n",
    "**First, we'll define the chain we wish to run over the dataset.**\n",
    "\n",
    "In this case, we're using an agent, but it can be any simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b59104-b90e-466a-b7ea-c5bd0194263b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84094a4a-1d76-461c-bc37-8c537939b466",
   "metadata": {},
   "source": [
    "**Now we're ready to run the chain!**\n",
    "\n",
    "The docstring below hints ways you can configure the method to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112d7bdf-7e50-4c1a-9285-5bac8473f2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marun_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm_or_chain_factory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'MODEL_OR_CHAIN_FACTORY'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconcurrency_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_repetitions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msession_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Dict[str, Any]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run the chain on a dataset and store traces to the specified session name.\n",
       "\n",
       "Args:\n",
       "    dataset_name: Name of the dataset to run the chain on.\n",
       "    llm_or_chain_factory: Language model or Chain constructor to run\n",
       "        over the dataset. The Chain constructor is used to permit\n",
       "        independent calls on each example without carrying over state.\n",
       "    concurrency_level: The number of async tasks to run concurrently.\n",
       "    num_repetitions: Number of times to run the model on each example.\n",
       "        This is useful when testing success rates or generating confidence\n",
       "        intervals.\n",
       "    session_name: Name of the session to store the traces in.\n",
       "        Defaults to {dataset_name}-{chain class name}-{datetime}.\n",
       "    verbose: Whether to print progress.\n",
       "\n",
       "Returns:\n",
       "    A dictionary mapping example ids to the model outputs.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/lc/lckg/langchain/client/langchain.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?client.arun_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e10f823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since chains can be stateful (e.g. they can have memory), we need provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "chain_factory = lambda: initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False,\n",
    "    # We will use the intermediate steps later for evaluation\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "# If your chain is NOT stateful, your lambda can return the object directly\n",
    "# to improve runtime performance. For example:\n",
    "# chain_factory = lambda: agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8088b7d-3ab6-4279-94c8-5116fe7cee33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed examples: 6\r"
     ]
    }
   ],
   "source": [
    "evaluation_session_name = \"Search + Calculator Agent Evaluation\"\n",
    "chain_results = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain_factory,\n",
    "    concurrency_level=5, # Optional, sets the number of examples to run at a time\n",
    "    verbose=True,\n",
    "    session_name=evaluation_session_name # Optional, a unique session name will be generated if not provided\n",
    ")\n",
    "\n",
    "# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n",
    "# These are logged as warnings here and captured as errors in the tracing UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd159-eb4d-49e9-bb2a-c55322c40ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reviewing the Chain Results\n",
    "\n",
    "You can review the results of the run in the tracing UI below and navigating to the session \n",
    "with the title **\"Search + Calculator Agent Evaluation\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136db492-d6ca-4215-96f9-439c23538241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost\", target=\"_blank\" rel=\"noopener\">LangChain+ Client</a>"
      ],
      "text/plain": [
       "LangChainPlusClient (API URL: http://localhost:8000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can navigate to the UI by clicking on the link below\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed6561-6574-43b3-a653-fe410aa8a617",
   "metadata": {},
   "source": [
    "## Running an Evaluation Chain\n",
    "\n",
    "Manually comparing the results of chains in the UI is effective, but it can be time consuming.\n",
    "It's easier to leverage AI-assisted feedback to evaluate your agent's performance.\n",
    "\n",
    "A few ways of doing this include:\n",
    "- Adding ground-truth answers as outputs to the dataset and evaluating relative to those references.\n",
    "- Evaluating the overall agent trajectory based on the tool usage and intermediate steps.\n",
    "- Evaluating performance based on 'context' such as retrieved documents or tool results.\n",
    "- Evaluating 'aspects' of the agent's response in a reference-free manner using targeted agent prompts.\n",
    "    \n",
    "We will demonstrate the first two here. First, we will supply labels to the dataset.\n",
    "\n",
    "**Note: the feedback API is currently experimental and subject to change.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98212e82-712b-424c-9565-9244162d5400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/wfh/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--agent-search-calculator-8a025c0ce5fb99d2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8161e12bd6734de7bae23426b2169450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.evaluation.loading import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"agent-search-calculator\")\n",
    "df = pd.DataFrame(dataset, columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64724ccd-e27e-4c30-94f0-a612e3bb85d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = client.list_examples(dataset_name=dataset_name)\n",
    "examples_dict = {\n",
    "    example.inputs['input']: example.id for example in examples\n",
    "}\n",
    "for tup in df.itertuples():\n",
    "    if tup.question not in examples_dict:\n",
    "        continue\n",
    "    example_id = examples_dict[tup.question]\n",
    "    client.update_example(outputs={\"answer\": tup.answer}, example_id=example_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812bb4e2-e2c1-4381-ad0a-08c5f3079c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "chain = QAEvalChain.from_llm(eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35db4025-9183-4e5f-ba14-0b1b380f49c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the session that was auto-generated\n",
    "latest_session = client.read_session(session_name=evaluation_session_name)\n",
    "runs = list(client.list_runs(session_id=latest_session.id, execution_order=1, error=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e628c65-420a-40ce-9aee-ff93d60fe795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "answers = [client.read_example(example_id=run.reference_example_id).outputs[\"answer\"] for run in runs]\n",
    "inputs = [{**run.inputs, \"answer\": answer} for run, answer in zip(runs, answers)]\n",
    "outputs = [{\"output\": run.outputs[\"output\"]} for run in runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb9900a8-0c32-45ec-a09a-0832ead7423c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = chain.evaluate(\n",
    "    examples = inputs,\n",
    "    predictions=outputs,\n",
    "    question_key=\"input\",\n",
    "    answer_key=\"answer\",\n",
    "    prediction_key=\"output\",\n",
    ")\n",
    "\n",
    "for run, result in zip(runs, results):\n",
    "    client.create_feedback(run.id, \"grade\", result[\"text\"], source_info={\"evaluator\": \"QAEvalChain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd03c-8769-4640-a340-02a35c658ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating Agent Trajectories\n",
    "\n",
    "Often ground-truth reference labels aren't available to grade your\n",
    "model's performance. In this case, there are a variety of \"reference-free\" evaluation\n",
    "techniques that can be used.\n",
    "\n",
    "One of these is the `TrajectoryEvalChain`, which scores the agent based on logical use\n",
    "of its tools and how helpful the ultimate answer is.\n",
    "\n",
    "This is just one example of how to you can evaluate your agent's behavior, and we encourage\n",
    "you to develop your own chains to grade your agent along other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84c52ee6-8f6f-4f26-bb82-c343bd0c91f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.agents import TrajectoryEvalChain\n",
    "\n",
    "eval_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "eval_chain = TrajectoryEvalChain.from_llm(\n",
    "    llm=eval_llm,\n",
    "    agent_tools=agent.tools,\n",
    "    return_reasoning=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c03142c-d1dc-4ba2-9f3c-99ebeddf9c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the session used for the latest runs\n",
    "latest_session = client.read_session(session_name=evaluation_session_name)\n",
    "runs = client.list_runs(session_id=latest_session.id, execution_order=1, error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f9f81f-c7dd-42f7-a169-c70be00e986e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c27a95063d4192a1716d5d14d00c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema import AgentAction\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_intermediate_steps(run):\n",
    "    \"\"\"Convert the serialized steps into an object.\"\"\"\n",
    "    results = []\n",
    "    for action, observation in run.outputs['intermediate_steps']:\n",
    "        results.append([AgentAction(*action), observation])\n",
    "    return results\n",
    "\n",
    "for run in tqdm(runs):\n",
    "    evaluation = eval_chain(\n",
    "        inputs={\n",
    "            \"question\": run.inputs['input'],\n",
    "            \"answer\": run.outputs['output'],\n",
    "            \"agent_trajectory\": eval_chain.get_agent_trajectory(get_intermediate_steps(run))\n",
    "        },\n",
    "    )\n",
    "    client.create_feedback(\n",
    "        run.id, \n",
    "        \"trajectory_evaluation\",\n",
    "        evaluation[\"score\"], \n",
    "       source_info={\n",
    "           \"evaluation_model\": \"gpt-4\",\n",
    "            \"evaluation_chain\": \"TrajectoryEvalChain\"\n",
    "       }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7a975-4223-4f09-b27f-589cbdb71f99",
   "metadata": {},
   "source": [
    "**Now the evaluation feedback is saved for future analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cceb5-aa53-4851-bb12-386f092191f9",
   "metadata": {},
   "source": [
    "## Running a Chat Model over a Traced Dataset\n",
    "\n",
    "We've shown how to run a _chain_ over a dataset, but you can also run an LLM or Chat model over a datasets formed from runs. \n",
    "\n",
    "First, we'll show an example using a ChatModel. This is useful for things like:\n",
    "- Comparing results under different decoding parameters\n",
    "- Comparing model providers\n",
    "- Testing for regressions in model behavior\n",
    "- Running multiple times with a temperature to gauge stability \n",
    "\n",
    "To speed things up, we'll upload a dataset we've previously captured directly to the tracing service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64490d7c-9a18-49ed-a3ac-36049c522cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/wfh/.cache/huggingface/datasets/LangChainDatasets___parquet/LangChainDatasets--two-player-dnd-cc62c3037e2d9250/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef824a19bcf4e0d8491898ee2eacc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generations</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[{'generation_info': None, 'message': {'conte...</td>\n",
       "      <td>[{'data': {'content': 'Here is the topic for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[{'generation_info': None, 'message': {'conte...</td>\n",
       "      <td>[{'data': {'content': 'Here is the topic for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[{'generation_info': None, 'message': {'conte...</td>\n",
       "      <td>[{'data': {'content': 'Here is the topic for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[{'generation_info': None, 'message': {'conte...</td>\n",
       "      <td>[{'data': {'content': 'Here is the topic for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[{'generation_info': None, 'message': {'conte...</td>\n",
       "      <td>[{'data': {'content': 'Here is the topic for a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         generations   \n",
       "0  [[{'generation_info': None, 'message': {'conte...  \\\n",
       "1  [[{'generation_info': None, 'message': {'conte...   \n",
       "2  [[{'generation_info': None, 'message': {'conte...   \n",
       "3  [[{'generation_info': None, 'message': {'conte...   \n",
       "4  [[{'generation_info': None, 'message': {'conte...   \n",
       "\n",
       "                                            messages  \n",
       "0  [{'data': {'content': 'Here is the topic for a...  \n",
       "1  [{'data': {'content': 'Here is the topic for a...  \n",
       "2  [{'data': {'content': 'Here is the topic for a...  \n",
       "3  [{'data': {'content': 'Here is the topic for a...  \n",
       "4  [{'data': {'content': 'Here is the topic for a...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.evaluation.loading import load_dataset\n",
    "\n",
    "chat_dataset = load_dataset(\"two-player-dnd\")\n",
    "chat_df = pd.DataFrame(chat_dataset)\n",
    "chat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "348acd86-a927-4d60-8d52-02e64585e4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_dataset_name = \"two-player-dnd\"\n",
    "\n",
    "if chat_dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
    "    client.upload_dataframe(chat_df, \n",
    "                            name=chat_dataset_name,\n",
    "                            description=\"An example dataset traced from chat models in a multiagent bidding dialogue\",\n",
    "                            input_keys=[\"messages\"],\n",
    "                            output_keys=[\"generations\"],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a43b8-e4f9-4220-b75d-33e310bc318b",
   "metadata": {},
   "source": [
    "### Reviewing behavior with temperature\n",
    "\n",
    "Here, we will set `num_repetitions > 1` and set the temperature to 0.3 to see the variety of response types for a each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cd3af25-474e-4de7-8ba8-d63b01c4f6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install anthropic > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a69dd183-ad5e-473d-b631-db90706e837f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "chat_model = ChatAnthropic(temperature=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "063da2a9-3692-4b7b-8edb-e474824fe416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed examples: 36\r"
     ]
    }
   ],
   "source": [
    "chat_model_results = await client.arun_on_dataset(\n",
    "    dataset_name=chat_dataset_name,\n",
    "    llm_or_chain_factory=chat_model,\n",
    "    concurrency_level=5, # Optional, sets the number of examples to run at a time\n",
    "    num_repetitions=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# The 'experimental tracing v2' warning is expected, as we are still actively developing the v2 tracing API \n",
    "# Since we are running examples concurrently,  you may run into some RateLimit warnings from your model\n",
    "# provider. In most cases, the tests will still run to completion (the wrappers have backoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bfe08-215c-4328-b9b0-631d9a41f0e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reviewing the Chat Model Results\n",
    "\n",
    "You can review the latest runs by clicking on the link below and navigating to the \"two-player-dnd\" session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b7a81f2-d19d-438b-a4bb-5678f746b965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost\", target=\"_blank\" rel=\"noopener\">LangChain+ Client</a>"
      ],
      "text/plain": [
       "LangChainPlusClient (API URL: http://localhost:8000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896cbeb-345f-430b-ab5e-e108973174f8",
   "metadata": {},
   "source": [
    "## Running an LLM over a Traced Dataset\n",
    "\n",
    "You can run an LLM over a dataset in much the same way as the chain and chat models, provided the dataset you've captured is in the appropriate format. We've cached one for you here, but using application-specific traces will be much more useful for your use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6805d0b-4612-4671-bffb-e6978992bd40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='text-curie-001', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d7cb243-40c3-44dd-8158-a7b910441e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/wfh/.cache/huggingface/datasets/LangChainDatasets___parquet/LangChainDatasets--state-of-the-union-completions-5347290a406c64c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5fb1d4dc6748b893227e637ddcd8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generations</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[{'generation_info': {'finish_reason': 'stop'...</td>\n",
       "      <td>The pandemic has been punishing. \\n\\nAnd so ma...</td>\n",
       "      <td>Putin may circle Kyiv with tanks, but he will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[]]</td>\n",
       "      <td>With a duty to one another to the American peo...</td>\n",
       "      <td>Madam Speaker, Madam Vice President, our First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[{'generation_info': {'finish_reason': 'stop'...</td>\n",
       "      <td>He thought he could roll into Ukraine and the ...</td>\n",
       "      <td>With a duty to one another to the American peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[]]</td>\n",
       "      <td>And the costs and the threats to America and t...</td>\n",
       "      <td>Please rise if you are able and show that, Yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[{'generation_info': {'finish_reason': 'stop'...</td>\n",
       "      <td>Please rise if you are able and show that, Yes...</td>\n",
       "      <td>Groups of citizens blocking tanks with their b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         generations   \n",
       "0  [[{'generation_info': {'finish_reason': 'stop'...  \\\n",
       "1                                               [[]]   \n",
       "2  [[{'generation_info': {'finish_reason': 'stop'...   \n",
       "3                                               [[]]   \n",
       "4  [[{'generation_info': {'finish_reason': 'stop'...   \n",
       "\n",
       "                                        ground_truth   \n",
       "0  The pandemic has been punishing. \\n\\nAnd so ma...  \\\n",
       "1  With a duty to one another to the American peo...   \n",
       "2  He thought he could roll into Ukraine and the ...   \n",
       "3  And the costs and the threats to America and t...   \n",
       "4  Please rise if you are able and show that, Yes...   \n",
       "\n",
       "                                              prompt  \n",
       "0  Putin may circle Kyiv with tanks, but he will ...  \n",
       "1  Madam Speaker, Madam Vice President, our First...  \n",
       "2  With a duty to one another to the American peo...  \n",
       "3  Please rise if you are able and show that, Yes...  \n",
       "4  Groups of citizens blocking tanks with their b...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions_dataset = load_dataset(\"state-of-the-union-completions\")\n",
    "completions_df = pd.DataFrame(completions_dataset)\n",
    "completions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7dcc1b2-7aef-44c0-ba0f-c812279099a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "completions_dataset_name = \"state-of-the-union-completions\"\n",
    "\n",
    "if completions_dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
    "    client.upload_dataframe(completions_df, \n",
    "                            name=completions_dataset_name,\n",
    "                            description=\"An example dataset traced from completion endpoints over the state of the union address\",\n",
    "                            input_keys=[\"prompt\"],\n",
    "                            output_keys=[\"generations\"],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e946138e-bf7c-43d7-861d-9c5740c933fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 processed\r"
     ]
    }
   ],
   "source": [
    "# We also offer a synchronous method for running examples if a chain or llm's async methods aren't yet implemented\n",
    "completions_model_results = client.run_on_dataset(\n",
    "    dataset_name=completions_dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    num_repetitions=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86e8e6-cee2-429e-942b-289284d14816",
   "metadata": {},
   "source": [
    "## Reviewing the LLM Results\n",
    "\n",
    "You can once again inspect the latest runs by clicking on the link below and navigating to the \"two-player-dnd\" session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf96f17-74c1-4f7d-8458-ae5ab5c6bd36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost\", target=\"_blank\" rel=\"noopener\">LangChain+ Client</a>"
      ],
      "text/plain": [
       "LangChainPlusClient (API URL: http://localhost:8000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80cd88-cd6f-4fdc-965f-f74600e1f286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
